\section{Problem Definition \& Methodology}
\label{sec:rerl}
We frame the problem of run time enforcement as a sequential decision making (SDM) one, by which the BIP engine has to be guided to select the set of interactions over an extended execution traces that maximize a cumulative return.  We formalize SDMs as the following five-tuple $\left \langle Q, \tilde{Q},\Gamma, \rightarrow, {R}_{+}, {R}_{-}, \gamma \right\rangle$.  Here, $Q$ represents the set of all possible states, $\tilde{Q} \subseteq Q$ the set of ``bad'' states that need to be avoided, $\Gamma$ the set of allowed interactions, and $\rightarrow$ represents the transition model. $R_{+}$ and ${R}_{-}$ are two positive and negative scalar parameters, which allow us to define the reward function quantifying the selection of the engine.
Clearly, the engine gets rewarded when in a state $q \notin \tilde{Q}$, while penalized if $q \in \tilde{Q}$. Using this intuition, one can define a reward function of the states written as: 


\begin{displaymath}
   \mathcal{R}(q) = \left\{
     \begin{array}{lr}
       R_{+} & :  q \notin \tilde{Q} \\
       R_{-} & : q \in \tilde{Q}.
     \end{array}
   \right.
\end{displaymath} 
Given the above reward definition, we finally introduce $\gamma \in [0,1)$ to denote the discount factor specifying the degree to which rewards are discounted over time as the engine interacts with each of the components. 

At each time step $t$, the engine observes a state $q_{t} \in Q$ and must choose an interaction $a_{t} \in \mathcal{A}_{q_t} \subseteq \Gamma$, transitioning it to a new state $q_{t} \goesto[a_{t}]q_{t+1}$ as given by $\rightarrow$ and yielding a reward $\mathcal{R}\left(q_{t+1}\right)$, where $\mathcal{A}_{q_t}$ denotes all enabled interactions from state  $q_{t}$, i.e., $\mathcal{A}_{q_t} = \{a \mid \exists q': q_{t} \goesto[a] q' \in \rightarrow\}$. 
We filter the choice of the allowed interactions, i.e., $\mathcal{A}_{q_t}$, at each time-step by an interaction-selection rule, which we refer to as the policy $\pi$. We extend the sequential decision making literature by defining policies that map between the set of states, $Q$, and any combination of the allowed interactions, i.e., $\pi: Q \rightarrow 2^{\Gamma}$, where for all $q \in Q: \pi(q) \subseteq\mathcal{A}_{q}$.
Consequently, the new behavior of the composite component, guided by the policy $\pi$, is defined by $C_\pi = (Q, \Gamma, \goesto_\pi)$, where $\goesto_\pi$ is the least set of transitions satisfying the following rule:
\begin{mathpar}
\inferrule*
	{
      q \goesto[a] q' \and
      a \in \pi(q)
    }
    {
      q \goesto[a]_\pi q'
    }
\end{mathpar}

The goal now is to find an optimal policy $\pi^{\star}$ that maximizes the \emph{expected} total sum of the rewards it receives in the long run, while starting from an initial state $q_{0} \in Q$. We will evaluate the performance of a policy $\pi$ by:
\begin{equation}
\label{Eq:ValueOne}
\texttt{eval}({\pi}|q_{0}) = \mathbb{E}_{\bm{\rho}(C_\pi)} \left[\sum_{t=0}^{T} \gamma^{t}\mathcal{R}(q_{t},a_{t})\right]= \mathbb{E}_{\bm{\rho}(C_\pi)} \left[\sum_{t=0}^{T} \gamma^{t}\mathcal{R}(q_{t+1})\right],
\end{equation}
where  $\mathbb{E}_{\bm{\rho}(C_\pi)}$ denotes the expectation under all the set of all the allowed (by the policy $\pi$) possible traces, and $T$ is the length of the trace. 


Notice that we index the value of the state by the policy $\pi$ to explicitly reflect the dependency of the value on the policy being followed from a state $q_{t}$. Interestingly, the definition of the evaluator asserts that the value of a state $q_{t}$ is the expected instantaneous reward plus the expected discounted value of the next state. Clearly, we are interested in determining the optimal policy $\pi^{\star}$, which upon its usage yields maximized values for any $q_{t} \in Q$. As such our goal is to determine a policy $\pi$ that solves:
\begin{equation*}
\pi^{\star} \equiv \max_{\pi} \texttt{eval}({\pi}|q_{0}). 
\end{equation*}

%In other words, rather than targeting the determination of $\pi^{\star}$ directly, we will seek the \emph{value} of a state $q \in Q$. %Clearly, one can recover the current iteration policy $\pi$ using:
%$
%\pi(q) = \texttt{argmax}_{a}\{V(q') \mid q \goesto[a] q'\}
%$.


Finally, we quantify the performance of the engine being in a state $q_{t}$ using the so-called value function $V: {Q} \rightarrow \mathbb{R}$. Given such a performance measure, we can deduce the policy, $\pi$ as: 
\begin{equation}
\label{Eq:Policy}
\pi(q) = \arg\max_{a} \{V(q') \mid q \goesto[a] q'\}.
\end{equation}

In what comes next, we define two methods capable of computing such performance measures (consequently policies) in finite as well as infinite state-space. 

\subsection{Finite State-Space - Value Iteration}
Due to the number of possible policies at each time step, it is a challenge to compute the value for all possible options. Instead, we propose the application of a dynamic programming algorithm known as value iteration, summarized in Algorithm~\ref{Algo:VI} to find the optimal policy efficiently. 
\begin{algorithm}[h!]
\caption{Value Iteration for Run Time Enforcement}
\label{Algo:VI}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Initialization of $V_{0}(q)$ for all $q\in Q$, precision parameter $\epsilon$
\WHILE {$|V_{k+1} - V_{k}| \leq \epsilon$}
	\FOR {\textbf{each} $q \in Q$ }
		\FOR {\textbf{each} $a \in \mathcal{A}$}
			\STATE $V_{k+1}(q) = \max_{a} \left[\mathcal{R}(q, a) + \gamma \sum_{q \goesto[a]q^{\prime}}V_{k}(q^{\prime})\right]$
		\ENDFOR
	\ENDFOR
		\STATE k = k+1
\ENDWHILE
\end{algorithmic}
\end{algorithm}

In essence, Algorithm~\ref{Algo:VI} is iteratively updating the value for all states by choosing these actions that maximize the instantaneous rewards, as well as the future information encoded through $V_{k}(q^{\prime})$. Defining the operator $$\mathcal{T}^{\pi} = \max_{a} \left[\mathcal{R}(q, a) + \gamma \sum_{q \goesto[a]q^{\prime}}V_{k}(q^{\prime})\right],$$ one can easily see that the optimal value function is essentially  fixed-point of $V_{k+1} = \mathcal{T}^{\pi}  V_{k}$. Contrary to state-space-exploration algorithms, our method remedies the need to construct the full labeled transition system as we only require the knowledge of the successor state from a given state and action pair with no regard to its reachability properties. Furthermore, notice that though line~3 in Algorithm~\ref{Algo:VI} requires a loop over all states computational time can be highly reduced by following a sampling-based procedure, where fractions of the state-space are considered. Notice, however, the successfulness of the attained policy comes hand-in-hand with the fraction of the state space sampled. In other words, the higher the fraction, the closer to optimality is the policy and vice versa. 


\subsection{Infinite State-Space - Deep Value Iteration}
The methodology detailed so-far suffers when considering infinite state-spaces as it requires exact representation of performance measures and policies. In general, an exact representation can only be achieved by storing distinct estimates of the return for every state-interaction pair. When states are continuous, such exact representations are no longer possible and performance measures need to be represented approximately. 

Approximation in the continuous setting is not only a problem of representation. Two additional types of approximation are needed. Firstly, sample-based approximation is necessary in any of these frameworks. Secondly, the algorithm must repeatedly solve a difficult minimization problem. To clarify, consider Algorithm~\ref{Algo:VI}, where every iteration necessitates a loop over \emph{every state-interaction pair}. When state space contains an infinite number of elements, it is impossible to loop over all pairs in finite time. Instead, a sample-based update that only considers a finite number of such pairs has to be used. 
%Given these collected samples, the algorithm has then to update the approximate performance measure; a step that entails 

%The usage of sample-based methods in combination with standard machine learning approximators pose additional problems that need to be carefully analyzed. 
%\textbf{Write about experience replay idea, i.e., forgetting and representation} 
In this section, our goal is to develop an algorithm capable of avoiding the problems above. This ultimately leads us to a method for run-time enforcement operating in continuous state spaces. 
To commence, we introduce a function approximator, encoded through a neural network (particularly its weight matrices which we denote by $\bm{\theta}$), to represent state-values. The goal of this approximator is to \emph{autonomously generalize} over the state-space, such that similarly behaving states cluster together. To achieve the aforementioned goal, one is to fit the parameterization, $\bm{\theta}$, of the neural network to obtain high-expected return.  Unfortunately, the direct application of neural networks in the context of run-time enforcement is challenging since the engine itself has to build, through sampling, a \emph{labeled} data set (with states being inputs and state-values as outputs) to train on. To worsen the problem, at no time instance the engine is given the optimal value of a state $q_{t}$. It rather has to determine these through exploring an infinite state-space. 


\begin{algorithm}[h!]
\caption{Deep-Value Iteration for Run Time Enforcement in Continuous States}
\label{Algo:VI2}
\begin{algorithmic}[1]
\STATE Initialize replay memory $\mathcal{D}$ to capacity $N$, and the neural network weights randomly 
\FOR {$\text{episode} = 1 \ \ \text{to} \ \ M$}
\STATE Set initial state to $q_{0}$
\FOR  {$t=1$ \ \ \text{to} \ \ T}
\STATE With some probability $\epsilon$ select a random interaction $a_{t}$
\STATE With a probability $1 - \epsilon$ select $a_{t} = \max_{a} \left[\mathcal{R}(q_{t}, a) + \gamma V(q_{t+1};\bm{\theta})\right]$
\STATE Execute interaction $a_{t}$ and observe reward, $r_{t}$,and successor state $q_{t+1}$
\STATE Store transition $\left(q_{t}, a_{t}, r_{t}, q_{t+1}\right)$ on replay memory $\mathcal{D}$
\STATE Sample random minibatch of transitions $\left(q_{j}, a_{j}, r_{j}, q_{j+1}\right)$ of size $N_{2}$ from $\mathcal{D}$ and create output label by 
\begin{displaymath}
   y_{j} = \left\{
     \begin{array}{lr}
       r_{j} & \text{if $q_{j+1}$ is a bad state}\\
        r_{j} + \gamma \max_{a^{\prime}}\left[\mathcal{R}(q_{j+1}, a^{\prime}) + \gamma V(q_{j+1};\bm{\theta})\right]  & \text{if $q_{j+1}$ is a correct state}
     \end{array}
   \right.
\end{displaymath} 
\ENDFOR
\STATE Retrain network on the $N_{2}$ data points with $y_{j}$ being the labels. 
\ENDFOR
\end{algorithmic}
\end{algorithm}
%The details of the above process are described in 
Our solution to the above problem is summarized in Algorithm~\ref{Algo:VI2} . On a high-level, the algorithm operates in two loops. The first is episode-based, while the second runs for a horizon $T$. 
At each episode, the goal is to collect relevant labeled data, encoding a trace of the system, to improve the approximation -- encoded through the neural network -- of the performance measure, as summarized in lines 5-10 in the algorithm. A trace is selected as follows. First, the algorithm selects an interaction either randomly with a probability $\epsilon$ (i.e., exploration) or by exploiting the current estimate of the performance measure. Next, the engine executes the interaction and stores both the dynamical transition and its usefulness (i.e., reward) in a replay memory data set, $\mathcal{D}$. To ensure that the learning algorithm takes learning memory into account, we sample a set of size $N_{2}$ from $\mathcal{D}$ and label them in line~9  in preparation to retrain the original neural network. The process by which we generate these labels is in essence similar to finite value iterator. The main difference, however, is the usage of sample-based transitions to train a neural network.  

%In other words, starting from randomly initialized weights, $\bm{\theta}$, the engine has to 1) gather relevant traces from the environment, and 2) improve its estimate of $\bm{\theta}$ to maximize its return. Having 

%The goal of such a neural net is to enable continuous states by autonomously generalizing similar...


\subsection{Fairness}
Deep value iteration allows to compute $\bm{\theta}$, and hence $V(q_{t};\bm{\theta})$, for a given state $q_t$. As defined in Equation~\ref{Eq:Policy}, the policy can be defined using $V$. For this, as we are dealing with real numbers, the same trace would be selected by the policy all the time. As such, other correct traces will not be reachable in the obtained system. For instance, given a global state, a policy would select the interaction leading to a state with maximum value function value, even though there exist other interactions leading to other correct traces. To remedy this, we define a fair policy that is allowed to deviate from the optimal policy with a degree of fairness.  The fair policy is defined as follows. 
\begin{equation}
\label{Eq:Policy-fair}
\pi(q) = \{a \mid q \goesto[a] q' \wedge V(q';\bm{\theta}) \geq \mathtt{max_q} - \mathtt{fair}\},
\end{equation}
where, $\mathtt{max_q} = \mathtt{maximum}\{V(q'') \mid q \goesto[a] q''\}$.  $\mathtt{fair}$ is the tolerance against the optimal policy. The value of $\mathtt{fair}$ depends on (1) the value of good and bad rewards, and (2) the  horizon used in deep value iteration algorithm. Clearly, the more fairness the more deviation from the optimal policy we get. 